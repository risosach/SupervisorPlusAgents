"""
OpenAI Chat Completions API Adapter for Supervisor Agent.

This module provides an OpenAI-compatible interface for the Supervisor Agent,
allowing it to be used as a drop-in replacement for OpenAI's Chat Completions API.

The adapter:
- Accepts OpenAI Chat Completions request format
- Routes queries through the Supervisor's existing routing logic
- Returns responses in OpenAI Chat Completions response format
- Maintains compatibility with existing Supervisor functionality

Examples:
    >>> from supervisor.api import create_chat_completion, ChatCompletionRequest
    >>> request = ChatCompletionRequest(
    ...     messages=[{"role": "user", "content": "What is the capital of France?"}],
    ...     model="claude-3-5-haiku-20241022"
    ... )
    >>> response = create_chat_completion(request, config_path='config.json')
    >>> print(response.choices[0].message.content)
    'The capital of France is Paris.'
"""

from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
from datetime import datetime
import time
from supervisor.agent import SupervisorAgent


class ChatMessage(BaseModel):
    """A single message in a chat conversation."""
    role: Literal["system", "user", "assistant"] = Field(
        description="The role of the message author"
    )
    content: str = Field(
        description="The content of the message"
    )


class ChatCompletionRequest(BaseModel):
    """
    OpenAI Chat Completions API request format.

    This matches the OpenAI API schema for chat completions,
    allowing the Supervisor to be used as an OpenAI-compatible endpoint.

    Attributes:
        messages: List of chat messages in the conversation
        model: Model to use (from .env CLAUDE_RUNTIME_MODEL or default)
        temperature: Sampling temperature (0.0 to 2.0) - currently ignored
        max_tokens: Maximum tokens in response - currently ignored
        stream: Whether to stream the response - not yet supported
    """
    messages: List[ChatMessage] = Field(
        description="List of messages in the conversation"
    )
    model: Optional[str] = Field(
        default=None,
        description="Model to use (e.g., claude-3-5-haiku-20241022)"
    )
    temperature: Optional[float] = Field(
        default=1.0,
        ge=0.0,
        le=2.0,
        description="Sampling temperature between 0 and 2"
    )
    max_tokens: Optional[int] = Field(
        default=None,
        description="Maximum number of tokens in the response"
    )
    stream: Optional[bool] = Field(
        default=False,
        description="Whether to stream the response (not yet supported)"
    )


class ChatCompletionChoice(BaseModel):
    """A single completion choice from the model."""
    index: int = Field(
        description="The index of this choice in the list"
    )
    message: ChatMessage = Field(
        description="The message generated by the model"
    )
    finish_reason: Optional[str] = Field(
        default="stop",
        description="Why the model stopped generating (stop, length, etc.)"
    )


class ChatCompletionUsage(BaseModel):
    """Token usage information for the completion."""
    prompt_tokens: int = Field(
        description="Number of tokens in the prompt"
    )
    completion_tokens: int = Field(
        description="Number of tokens in the completion"
    )
    total_tokens: int = Field(
        description="Total tokens used"
    )


class ChatCompletionResponse(BaseModel):
    """
    OpenAI Chat Completions API response format.

    This matches the OpenAI API schema for chat completion responses.

    Attributes:
        id: Unique identifier for this completion
        object: Always "chat.completion"
        created: Unix timestamp when the completion was created
        model: Model used for the completion
        choices: List of completion choices
        usage: Token usage information
    """
    id: str = Field(
        description="Unique identifier for this completion"
    )
    object: Literal["chat.completion"] = Field(
        default="chat.completion",
        description="Object type"
    )
    created: int = Field(
        description="Unix timestamp of when the completion was created"
    )
    model: str = Field(
        description="Model used for the completion"
    )
    choices: List[ChatCompletionChoice] = Field(
        description="List of completion choices"
    )
    usage: Optional[ChatCompletionUsage] = Field(
        default=None,
        description="Token usage information"
    )


def create_chat_completion(
    request: ChatCompletionRequest,
    config_path: str = "config.json"
) -> ChatCompletionResponse:
    """
    Create a chat completion using the Supervisor Agent.

    This function provides an OpenAI-compatible interface to the Supervisor Agent.
    It accepts a ChatCompletionRequest and returns a ChatCompletionResponse.

    Args:
        request: OpenAI-formatted chat completion request
        config_path: Path to Supervisor configuration file

    Returns:
        ChatCompletionResponse with the model's response

    Raises:
        ValueError: If the request is invalid or missing required fields
        Exception: Any errors from the Supervisor Agent

    Examples:
        >>> request = ChatCompletionRequest(
        ...     messages=[{"role": "user", "content": "What is 2+2?"}],
        ...     model="claude-3-5-haiku-20241022"
        ... )
        >>> response = create_chat_completion(request)
        >>> print(response.choices[0].message.content)
        '[Stub Claude API Response] Received query: What is 2+2?'
    """
    # Validate request
    if not request.messages:
        raise ValueError("Messages list cannot be empty")

    # Check for streaming (not yet supported)
    if request.stream:
        raise NotImplementedError("Streaming is not yet supported (Phase 6)")

    # Extract the last user message as the query
    # In a full implementation, we would use the entire conversation history
    user_messages = [msg for msg in request.messages if msg.role == "user"]
    if not user_messages:
        raise ValueError("No user messages found in request")

    query = user_messages[-1].content

    # Initialize Supervisor Agent
    supervisor = SupervisorAgent(config_path=config_path)

    # Get response from Supervisor
    response_text = supervisor.respond(query)

    # Estimate token usage (rough approximation)
    prompt_tokens = sum(len(msg.content.split()) for msg in request.messages)
    completion_tokens = len(response_text.split())
    total_tokens = prompt_tokens + completion_tokens

    # Build OpenAI-compatible response
    completion_response = ChatCompletionResponse(
        id=f"chatcmpl-{int(time.time() * 1000)}",
        object="chat.completion",
        created=int(time.time()),
        model=request.model or "claude-3-5-haiku-20241022",
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatMessage(
                    role="assistant",
                    content=response_text
                ),
                finish_reason="stop"
            )
        ],
        usage=ChatCompletionUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )

    return completion_response


def create_chat_completion_from_dict(
    request_dict: Dict[str, Any],
    config_path: str = "config.json"
) -> Dict[str, Any]:
    """
    Create a chat completion from a dictionary request.

    Convenience function for working with raw dictionaries instead of Pydantic models.

    Args:
        request_dict: Dictionary matching ChatCompletionRequest schema
        config_path: Path to Supervisor configuration file

    Returns:
        Dictionary matching ChatCompletionResponse schema

    Examples:
        >>> request = {
        ...     "messages": [{"role": "user", "content": "Hello"}],
        ...     "model": "claude-3-5-haiku-20241022"
        ... }
        >>> response = create_chat_completion_from_dict(request)
        >>> response['choices'][0]['message']['content']
        '[Stub Claude API Response] Received query: Hello'
    """
    request = ChatCompletionRequest(**request_dict)
    response = create_chat_completion(request, config_path=config_path)
    return response.model_dump()
